{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-1.4.1-py2.py3-none-any.whl (22 kB)\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-0.2.2-py3-none-any.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 133 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from requests_html) (2.24.0)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.15.0.tar.gz (29 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 17 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml>=2.1\n",
      "  Downloading lxml-4.5.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 71 kB/s eta 0:00:011     |████████████████████████████▊   | 4.9 MB 150 kB/s eta 0:00:04\n",
      "\u001b[?25hCollecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from pyppeteer>=0.0.14->requests_html) (1.25.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from pyppeteer>=0.0.14->requests_html) (4.47.0)\n",
      "Collecting websockets<9.0,>=8.1\n",
      "  Downloading websockets-8.1-cp38-cp38-manylinux2010_x86_64.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 83 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyee<8.0.0,>=7.0.1\n",
      "  Downloading pyee-7.0.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from requests->requests_html) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from requests->requests_html) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from requests->requests_html) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages (from w3lib->requests_html) (1.15.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.0.1-py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: fake-useragent, bs4, parse\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13487 sha256=9c2f770a502d06a88c14672137db7f146d350a87583ae4ebf267a7576dc6657a\n",
      "  Stored in directory: /home/tebogo/.cache/pip/wheels/a0/b8/b7/8c942b2c5be5158b874a88195116b05ad124bac795f6665e65\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=fe0755747e9c3e153c535bc4ec7b246d9d9b465fff97223ebbbee9e6694d0271\n",
      "  Stored in directory: /home/tebogo/.cache/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "  Building wheel for parse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for parse: filename=parse-1.15.0-py3-none-any.whl size=23709 sha256=260242f6d99769644991edbcaf030c6837f7c594f03f44160a979f7ea8c25994\n",
      "  Stored in directory: /home/tebogo/.cache/pip/wheels/c3/19/07/8dbdfbbe6659e4dd5664b57526f05cd87678d2a9b3f0b5091f\n",
      "Successfully built fake-useragent bs4 parse\n",
      "Installing collected packages: fake-useragent, soupsieve, beautifulsoup4, bs4, lxml, cssselect, pyquery, websockets, pyee, appdirs, pyppeteer, w3lib, parse, requests-html\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.9.1 bs4-0.0.1 cssselect-1.1.0 fake-useragent-0.1.11 lxml-4.5.2 parse-1.15.0 pyee-7.0.2 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 soupsieve-2.0.1 w3lib-1.22.0 websockets-8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
