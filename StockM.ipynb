{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Share Price Predictor\n",
    "\n",
    "## Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ticker: The stock we intend to predict, e.g Tesla would be TSLA.\n",
    "- n_steps: The timeframe we intend to use for training, e.g. 50 would use the last 50 days' prices.\n",
    "- scale: Boolean which indicates whether we are want to scale our share prices between 0 and 1 or not.\n",
    "- lookup_step: The number of days into the future which we intend to predict for.\n",
    "- The adjcolse, volume, open, high and low indicators will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size or the sequence length\n",
    "N_STEPS = 100\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 400\n",
    "# Apple stock market\n",
    "ticker = \"AAPL\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory for saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.3749e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00001: val_loss improved from inf to 0.00022, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 17s 134ms/step - loss: 8.3749e-04 - mean_absolute_error: 0.0197 - val_loss: 2.2464e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 2/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 5.4730e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00002: val_loss improved from 0.00022 to 0.00017, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 14s 115ms/step - loss: 5.4730e-04 - mean_absolute_error: 0.0151 - val_loss: 1.7473e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 3/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 5.3394e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00003: val_loss improved from 0.00017 to 0.00016, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 15s 119ms/step - loss: 5.3394e-04 - mean_absolute_error: 0.0151 - val_loss: 1.6364e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 4/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 5.9814e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00004: val_loss improved from 0.00016 to 0.00015, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 14s 116ms/step - loss: 5.9814e-04 - mean_absolute_error: 0.0158 - val_loss: 1.5268e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 5/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 4.8379e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00005: val_loss did not improve from 0.00015\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 4.8379e-04 - mean_absolute_error: 0.0146 - val_loss: 3.2512e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 6/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 3.9212e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00006: val_loss did not improve from 0.00015\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 3.9212e-04 - mean_absolute_error: 0.0132 - val_loss: 2.7578e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 7/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 3.2655e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00007: val_loss did not improve from 0.00015\n",
      "124/124 [==============================] - 14s 116ms/step - loss: 3.2655e-04 - mean_absolute_error: 0.0126 - val_loss: 2.8281e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 8/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 3.9875e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00008: val_loss did not improve from 0.00015\n",
      "124/124 [==============================] - 14s 116ms/step - loss: 3.9875e-04 - mean_absolute_error: 0.0133 - val_loss: 1.7318e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 9/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.8176e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00009: val_loss improved from 0.00015 to 0.00009, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 2.8176e-04 - mean_absolute_error: 0.0116 - val_loss: 8.9362e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 10/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 3.0991e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00010: val_loss improved from 0.00009 to 0.00006, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 23s 188ms/step - loss: 3.0991e-04 - mean_absolute_error: 0.0122 - val_loss: 6.1893e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 11/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.8119e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00011: val_loss did not improve from 0.00006\n",
      "124/124 [==============================] - 19s 151ms/step - loss: 2.8119e-04 - mean_absolute_error: 0.0119 - val_loss: 9.3698e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 12/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.8199e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00012: val_loss improved from 0.00006 to 0.00004, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 24s 192ms/step - loss: 2.8199e-04 - mean_absolute_error: 0.0118 - val_loss: 4.0892e-05 - val_mean_absolute_error: 0.0039\n",
      "Epoch 13/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.1237e-04 - mean_absolute_error: 0.0109\n",
      "Epoch 00013: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 20s 162ms/step - loss: 2.1237e-04 - mean_absolute_error: 0.0109 - val_loss: 3.5183e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 14/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.1202e-04 - mean_absolute_error: 0.0106\n",
      "Epoch 00014: val_loss did not improve from 0.00004\n",
      "124/124 [==============================] - 27s 221ms/step - loss: 2.1202e-04 - mean_absolute_error: 0.0106 - val_loss: 5.2943e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 15/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.7918e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00015: val_loss did not improve from 0.00004\n",
      "124/124 [==============================] - 17s 134ms/step - loss: 1.7918e-04 - mean_absolute_error: 0.0102 - val_loss: 3.9637e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 16/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.1439e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00016: val_loss did not improve from 0.00004\n",
      "124/124 [==============================] - 18s 143ms/step - loss: 2.1439e-04 - mean_absolute_error: 0.0105 - val_loss: 1.3072e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 17/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 2.3498e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00017: val_loss did not improve from 0.00004\n",
      "124/124 [==============================] - 16s 133ms/step - loss: 2.3498e-04 - mean_absolute_error: 0.0115 - val_loss: 6.6585e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 18/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.8160e-04 - mean_absolute_error: 0.010 - ETA: 0s - loss: 1.8151e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00018: val_loss improved from 0.00004 to 0.00003, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 17s 136ms/step - loss: 1.8151e-04 - mean_absolute_error: 0.0104 - val_loss: 2.5526e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 19/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.6552e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00019: val_loss improved from 0.00003 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 17s 136ms/step - loss: 1.6552e-04 - mean_absolute_error: 0.0100 - val_loss: 2.3951e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 20/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.7099e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00020: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 21s 166ms/step - loss: 1.7099e-04 - mean_absolute_error: 0.0103 - val_loss: 5.3387e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 21/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.8213e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00021: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 22s 179ms/step - loss: 1.8213e-04 - mean_absolute_error: 0.0105 - val_loss: 6.9053e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 22/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.8034e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00022: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 138ms/step - loss: 1.8034e-04 - mean_absolute_error: 0.0103 - val_loss: 3.4453e-05 - val_mean_absolute_error: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4492e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00023: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 122ms/step - loss: 1.4492e-04 - mean_absolute_error: 0.0094 - val_loss: 3.4392e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 24/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.7546e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00024: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 122ms/step - loss: 1.7546e-04 - mean_absolute_error: 0.0103 - val_loss: 6.8466e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 25/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.5842e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00025: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 1.5842e-04 - mean_absolute_error: 0.0098 - val_loss: 2.6602e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 26/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.9053e-04 - mean_absolute_error: 0.0109\n",
      "Epoch 00026: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.9053e-04 - mean_absolute_error: 0.0109 - val_loss: 2.6292e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 27/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.5979e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00027: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.5979e-04 - mean_absolute_error: 0.0103 - val_loss: 4.4218e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 28/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.5051e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00028: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.5051e-04 - mean_absolute_error: 0.0099 - val_loss: 2.7693e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 29/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4833e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00029: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.4833e-04 - mean_absolute_error: 0.0097 - val_loss: 5.1803e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 30/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4954e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00030: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 1.4954e-04 - mean_absolute_error: 0.0098 - val_loss: 2.5494e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 31/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.3488e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00031: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.3488e-04 - mean_absolute_error: 0.0095 - val_loss: 4.3541e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 32/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.3788e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00032: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.3788e-04 - mean_absolute_error: 0.0096 - val_loss: 4.0411e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 33/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2547e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00033: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 1.2547e-04 - mean_absolute_error: 0.0090 - val_loss: 5.3251e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 34/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.5331e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00034: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 133ms/step - loss: 1.5331e-04 - mean_absolute_error: 0.0098 - val_loss: 7.7904e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 35/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2062e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00035: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 1.2062e-04 - mean_absolute_error: 0.0089 - val_loss: 3.6544e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 36/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.5532e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00036: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.5532e-04 - mean_absolute_error: 0.0099 - val_loss: 2.8863e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 37/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2403e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00037: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 137ms/step - loss: 1.2403e-04 - mean_absolute_error: 0.0087 - val_loss: 1.9083e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 38/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.3053e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00038: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.3053e-04 - mean_absolute_error: 0.0093 - val_loss: 2.4582e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 39/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2593e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00039: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 138ms/step - loss: 1.2593e-04 - mean_absolute_error: 0.0090 - val_loss: 4.4346e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 40/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.3217e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00040: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 130ms/step - loss: 1.3217e-04 - mean_absolute_error: 0.0096 - val_loss: 1.0721e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 41/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4383e-04 - mean_absolute_error: 0.0099- ETA:\n",
      "Epoch 00041: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 1.4383e-04 - mean_absolute_error: 0.0099 - val_loss: 2.5091e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 42/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1837e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00042: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 1.1837e-04 - mean_absolute_error: 0.0089 - val_loss: 4.1479e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 43/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4090e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00043: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 1.4090e-04 - mean_absolute_error: 0.0094 - val_loss: 6.1953e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 44/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1387e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00044: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 139ms/step - loss: 1.1387e-04 - mean_absolute_error: 0.0087 - val_loss: 8.3564e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 45/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2421e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00045: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.2421e-04 - mean_absolute_error: 0.0090 - val_loss: 2.3738e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 46/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.3242e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00046: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.3242e-04 - mean_absolute_error: 0.0098 - val_loss: 4.2902e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 47/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2459e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00047: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.2459e-04 - mean_absolute_error: 0.0089 - val_loss: 2.0611e-05 - val_mean_absolute_error: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1147e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00048: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 15s 119ms/step - loss: 1.1147e-04 - mean_absolute_error: 0.0083 - val_loss: 2.0452e-05 - val_mean_absolute_error: 0.0039\n",
      "Epoch 49/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1135e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00049: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 18s 147ms/step - loss: 1.1135e-04 - mean_absolute_error: 0.0086 - val_loss: 1.9134e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 50/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4637e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00050: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 20s 163ms/step - loss: 1.4637e-04 - mean_absolute_error: 0.0098 - val_loss: 5.5700e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 51/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4210e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00051: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 132ms/step - loss: 1.4210e-04 - mean_absolute_error: 0.0092 - val_loss: 2.5910e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 52/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1540e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00052: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 131ms/step - loss: 1.1540e-04 - mean_absolute_error: 0.0086 - val_loss: 2.7603e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 53/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1702e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00053: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 23s 184ms/step - loss: 1.1702e-04 - mean_absolute_error: 0.0088 - val_loss: 6.9392e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 54/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1403e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00054: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 1.1403e-04 - mean_absolute_error: 0.0087 - val_loss: 7.8444e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 55/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2695e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00055: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 1.2695e-04 - mean_absolute_error: 0.0091 - val_loss: 9.0318e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 56/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.3211e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00056: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 142ms/step - loss: 1.3211e-04 - mean_absolute_error: 0.0096 - val_loss: 2.5852e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 57/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1518e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00057: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.1518e-04 - mean_absolute_error: 0.0089 - val_loss: 1.9193e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 58/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1374e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00058: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.1374e-04 - mean_absolute_error: 0.0088 - val_loss: 2.1928e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 59/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0640e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00059: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.0640e-04 - mean_absolute_error: 0.0084 - val_loss: 6.0476e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 60/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.9871e-05 - mean_absolute_error: 0.0083\n",
      "Epoch 00060: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 9.9871e-05 - mean_absolute_error: 0.0083 - val_loss: 2.8863e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 61/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2417e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00061: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.2417e-04 - mean_absolute_error: 0.0089 - val_loss: 4.7686e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 62/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1744e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00062: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1744e-04 - mean_absolute_error: 0.0087 - val_loss: 2.6212e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 63/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.4742e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00063: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 125ms/step - loss: 1.4742e-04 - mean_absolute_error: 0.0099 - val_loss: 9.1375e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 64/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1640e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00064: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 132ms/step - loss: 1.1640e-04 - mean_absolute_error: 0.0088 - val_loss: 1.9310e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 65/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3348e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00065: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 9.3348e-05 - mean_absolute_error: 0.0079 - val_loss: 7.1687e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 66/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0257e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00066: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.0257e-04 - mean_absolute_error: 0.0083 - val_loss: 9.5682e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 67/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1713e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00067: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1713e-04 - mean_absolute_error: 0.0088 - val_loss: 7.4920e-05 - val_mean_absolute_error: 0.0109\n",
      "Epoch 68/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2095e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00068: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 19s 149ms/step - loss: 1.2095e-04 - mean_absolute_error: 0.0089 - val_loss: 2.1791e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 69/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2294e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00069: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.2294e-04 - mean_absolute_error: 0.0089 - val_loss: 8.4330e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 70/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1998e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00070: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 20s 157ms/step - loss: 1.1998e-04 - mean_absolute_error: 0.0092 - val_loss: 2.2093e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 71/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2568e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00071: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 132ms/step - loss: 1.2568e-04 - mean_absolute_error: 0.0091 - val_loss: 7.6920e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 72/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2077e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00072: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 135ms/step - loss: 1.2077e-04 - mean_absolute_error: 0.0093 - val_loss: 4.6167e-05 - val_mean_absolute_error: 0.0079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1466e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00073: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 1.1466e-04 - mean_absolute_error: 0.0085 - val_loss: 6.6548e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 74/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2002e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00074: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 133ms/step - loss: 1.2002e-04 - mean_absolute_error: 0.0091 - val_loss: 3.1153e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 75/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1149e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00075: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 19s 154ms/step - loss: 1.1149e-04 - mean_absolute_error: 0.0088 - val_loss: 6.1091e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 76/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0908e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00076: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 1.0908e-04 - mean_absolute_error: 0.0084 - val_loss: 6.5777e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 77/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0688e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00077: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 17s 135ms/step - loss: 1.0688e-04 - mean_absolute_error: 0.0085 - val_loss: 2.4092e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 78/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7198e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00078: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 18s 143ms/step - loss: 9.7198e-05 - mean_absolute_error: 0.0081 - val_loss: 6.6060e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 79/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0942e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00079: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 21s 167ms/step - loss: 1.0942e-04 - mean_absolute_error: 0.0085 - val_loss: 2.5730e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 80/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1496e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00080: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 1.1496e-04 - mean_absolute_error: 0.0087 - val_loss: 4.5959e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 81/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0625e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00081: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 25s 198ms/step - loss: 1.0625e-04 - mean_absolute_error: 0.0086 - val_loss: 1.7229e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 82/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1829e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00082: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 20s 160ms/step - loss: 1.1829e-04 - mean_absolute_error: 0.0088 - val_loss: 3.9970e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 83/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1911e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00083: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.1911e-04 - mean_absolute_error: 0.0089 - val_loss: 2.1239e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 84/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1020e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00084: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1020e-04 - mean_absolute_error: 0.0086 - val_loss: 1.8056e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 85/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0572e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00085: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.0572e-04 - mean_absolute_error: 0.0083 - val_loss: 2.3997e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 86/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0231e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00086: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 1.0231e-04 - mean_absolute_error: 0.0083 - val_loss: 5.2557e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 87/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1546e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00087: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1546e-04 - mean_absolute_error: 0.0085 - val_loss: 5.4047e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 88/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1661e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00088: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1661e-04 - mean_absolute_error: 0.0086 - val_loss: 1.7184e-05 - val_mean_absolute_error: 0.0028\n",
      "Epoch 89/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7160e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00089: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 8.7160e-05 - mean_absolute_error: 0.0077 - val_loss: 4.8621e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 90/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1844e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00090: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 1.1844e-04 - mean_absolute_error: 0.0089 - val_loss: 6.8166e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 91/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1124e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00091: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1124e-04 - mean_absolute_error: 0.0087 - val_loss: 2.0978e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 92/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0335e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00092: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.0335e-04 - mean_absolute_error: 0.0084 - val_loss: 4.3248e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 93/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0402e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00093: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.0402e-04 - mean_absolute_error: 0.0082 - val_loss: 3.3171e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 94/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0211e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00094: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.0211e-04 - mean_absolute_error: 0.0083 - val_loss: 9.0958e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 95/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0822e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00095: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.0822e-04 - mean_absolute_error: 0.0084 - val_loss: 1.3393e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 96/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1373e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00096: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.1373e-04 - mean_absolute_error: 0.0087 - val_loss: 1.2769e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 97/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0603e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00097: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 16s 130ms/step - loss: 1.0603e-04 - mean_absolute_error: 0.0088 - val_loss: 1.6320e-05 - val_mean_absolute_error: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0395e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00098: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 121ms/step - loss: 1.0395e-04 - mean_absolute_error: 0.0083 - val_loss: 4.5065e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 99/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0450e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00099: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 1.0450e-04 - mean_absolute_error: 0.0086 - val_loss: 5.5364e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 100/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0266e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00100: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 19s 149ms/step - loss: 1.0266e-04 - mean_absolute_error: 0.0083 - val_loss: 7.1334e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 101/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1243e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00101: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 123ms/step - loss: 1.1243e-04 - mean_absolute_error: 0.0086 - val_loss: 2.8370e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 102/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0077e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00102: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 15s 123ms/step - loss: 1.0077e-04 - mean_absolute_error: 0.0081 - val_loss: 2.6796e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 103/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2901e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00103: val_loss did not improve from 0.00002\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 1.2901e-04 - mean_absolute_error: 0.0092 - val_loss: 7.6950e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 104/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1862e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00104: val_loss improved from 0.00002 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 20s 164ms/step - loss: 1.1862e-04 - mean_absolute_error: 0.0088 - val_loss: 1.4811e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 105/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5004e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00105: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 20s 159ms/step - loss: 9.5004e-05 - mean_absolute_error: 0.0080 - val_loss: 1.7653e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 106/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0798e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00106: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 1.0798e-04 - mean_absolute_error: 0.0084 - val_loss: 2.5124e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 107/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2199e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00107: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 23s 182ms/step - loss: 1.2199e-04 - mean_absolute_error: 0.0093 - val_loss: 1.8126e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 108/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.8132e-05 - mean_absolute_error: 0.0082\n",
      "Epoch 00108: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 152ms/step - loss: 9.8132e-05 - mean_absolute_error: 0.0082 - val_loss: 3.6079e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 109/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.8098e-05 - mean_absolute_error: 0.0083\n",
      "Epoch 00109: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 9.8098e-05 - mean_absolute_error: 0.0083 - val_loss: 2.4743e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 110/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0480e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00110: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 136ms/step - loss: 1.0480e-04 - mean_absolute_error: 0.0086 - val_loss: 9.7653e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 111/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2663e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00111: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 1.2663e-04 - mean_absolute_error: 0.0090 - val_loss: 2.3136e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 112/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.2076e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00112: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.2076e-04 - mean_absolute_error: 0.0086 - val_loss: 4.4565e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 113/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.1413e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00113: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.1413e-04 - mean_absolute_error: 0.0087 - val_loss: 1.7243e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 114/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5466e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00114: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 9.5466e-05 - mean_absolute_error: 0.0079 - val_loss: 3.5459e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 115/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0031e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00115: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 1.0031e-04 - mean_absolute_error: 0.0081 - val_loss: 1.1637e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 116/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0078e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00116: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 132ms/step - loss: 1.0078e-04 - mean_absolute_error: 0.0083 - val_loss: 4.3514e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 117/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0756e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00117: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 1.0756e-04 - mean_absolute_error: 0.0084 - val_loss: 1.2003e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 118/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.8896e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00118: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 9.8896e-05 - mean_absolute_error: 0.0080 - val_loss: 3.2548e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 119/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0296e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00119: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 125ms/step - loss: 1.0296e-04 - mean_absolute_error: 0.0081 - val_loss: 2.7345e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 120/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2505e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00120: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 9.2505e-05 - mean_absolute_error: 0.0080 - val_loss: 2.9026e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 121/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8832e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00121: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 8.8832e-05 - mean_absolute_error: 0.0080 - val_loss: 1.5505e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 122/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5803e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00122: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 118ms/step - loss: 9.5803e-05 - mean_absolute_error: 0.0080 - val_loss: 3.3537e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 123/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0566e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00123: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 117ms/step - loss: 1.0566e-04 - mean_absolute_error: 0.0085 - val_loss: 1.3636e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 124/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0034e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00124: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 119ms/step - loss: 1.0034e-04 - mean_absolute_error: 0.0082 - val_loss: 2.8742e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 125/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7712e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00125: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 118ms/step - loss: 9.7712e-05 - mean_absolute_error: 0.0078 - val_loss: 2.3853e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 126/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.9536e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00126: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 121ms/step - loss: 8.9536e-05 - mean_absolute_error: 0.0078 - val_loss: 3.5444e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 127/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0250e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00127: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 123ms/step - loss: 1.0250e-04 - mean_absolute_error: 0.0082 - val_loss: 2.5614e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 128/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0510e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00128: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 1.0510e-04 - mean_absolute_error: 0.0084 - val_loss: 9.0490e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 129/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.4068e-05 - mean_absolute_error: 0.0082\n",
      "Epoch 00129: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 9.4068e-05 - mean_absolute_error: 0.0082 - val_loss: 2.4741e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 130/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.9574e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00130: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 9.9574e-05 - mean_absolute_error: 0.0080 - val_loss: 5.3578e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 131/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6494e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00131: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 9.6494e-05 - mean_absolute_error: 0.0079 - val_loss: 2.2789e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 132/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7039e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00132: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 9.7039e-05 - mean_absolute_error: 0.0077 - val_loss: 3.3782e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 133/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0302e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00133: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 134ms/step - loss: 1.0302e-04 - mean_absolute_error: 0.0082 - val_loss: 1.9387e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 134/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6977e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00134: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 133ms/step - loss: 9.6977e-05 - mean_absolute_error: 0.0080 - val_loss: 6.7450e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 135/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0216e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00135: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 20s 159ms/step - loss: 1.0216e-04 - mean_absolute_error: 0.0082 - val_loss: 3.8905e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 136/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.9869e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00136: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 9.9869e-05 - mean_absolute_error: 0.0080 - val_loss: 4.6489e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 137/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0646e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00137: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 146ms/step - loss: 9.0646e-05 - mean_absolute_error: 0.0080 - val_loss: 1.7953e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 138/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0225e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00138: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 1.0225e-04 - mean_absolute_error: 0.0082 - val_loss: 2.0470e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 139/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7417e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00139: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 109ms/step - loss: 9.7417e-05 - mean_absolute_error: 0.0081 - val_loss: 3.1573e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 140/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2971e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00140: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 109ms/step - loss: 9.2971e-05 - mean_absolute_error: 0.0079 - val_loss: 2.1484e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 141/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0065e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00141: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 9.0065e-05 - mean_absolute_error: 0.0078 - val_loss: 1.7105e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 142/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1010e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00142: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.1010e-05 - mean_absolute_error: 0.0074 - val_loss: 1.5198e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 143/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7116e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00143: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.7116e-05 - mean_absolute_error: 0.0079 - val_loss: 5.9223e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 144/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.4775e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00144: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 9.4775e-05 - mean_absolute_error: 0.0081 - val_loss: 2.7705e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 145/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0173e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00145: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 1.0173e-04 - mean_absolute_error: 0.0084 - val_loss: 1.8182e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 146/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0131e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00146: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 1.0131e-04 - mean_absolute_error: 0.0081 - val_loss: 5.3717e-05 - val_mean_absolute_error: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.4411e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00147: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.4411e-05 - mean_absolute_error: 0.0080 - val_loss: 2.4936e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 148/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0009e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00148: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0009e-04 - mean_absolute_error: 0.0082 - val_loss: 2.7060e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 149/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6770e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00149: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 113ms/step - loss: 9.6770e-05 - mean_absolute_error: 0.0080 - val_loss: 5.0648e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 150/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3080e-05 - mean_absolute_error: 0.0082\n",
      "Epoch 00150: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.3080e-05 - mean_absolute_error: 0.0082 - val_loss: 1.7572e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 151/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0328e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00151: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 1.0328e-04 - mean_absolute_error: 0.0082 - val_loss: 4.8749e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 152/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2197e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00152: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.2197e-05 - mean_absolute_error: 0.0079 - val_loss: 3.3701e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 153/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.8063e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00153: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.8063e-05 - mean_absolute_error: 0.0081 - val_loss: 1.8827e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 154/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.1829e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00154: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 9.1829e-05 - mean_absolute_error: 0.0077 - val_loss: 2.3634e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 155/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0334e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00155: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.0334e-05 - mean_absolute_error: 0.0080 - val_loss: 1.8895e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 156/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0475e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00156: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 9.0475e-05 - mean_absolute_error: 0.0080 - val_loss: 2.0512e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 157/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7830e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00157: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 8.7830e-05 - mean_absolute_error: 0.0078 - val_loss: 1.1508e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 158/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3080e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00158: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 9.3080e-05 - mean_absolute_error: 0.0078 - val_loss: 2.3998e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 159/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8342e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00159: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.8342e-05 - mean_absolute_error: 0.0080 - val_loss: 2.8871e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 160/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0124e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00160: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0124e-04 - mean_absolute_error: 0.0082 - val_loss: 2.1567e-05 - val_mean_absolute_error: 0.0028\n",
      "Epoch 161/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.9094e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00161: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.9094e-05 - mean_absolute_error: 0.0079 - val_loss: 4.7974e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 162/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0727e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00162: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 9.0727e-05 - mean_absolute_error: 0.0077 - val_loss: 2.1393e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 163/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0426e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00163: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0426e-04 - mean_absolute_error: 0.0084 - val_loss: 2.2064e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 164/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8297e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00164: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 8.8297e-05 - mean_absolute_error: 0.0076 - val_loss: 4.3819e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 165/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8741e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00165: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.8741e-05 - mean_absolute_error: 0.0080 - val_loss: 1.8190e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 166/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.3498e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00166: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 8.3498e-05 - mean_absolute_error: 0.0076 - val_loss: 3.7736e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 167/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5035e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00167: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.5035e-05 - mean_absolute_error: 0.0076 - val_loss: 1.8681e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 168/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0047e-04 - mean_absolute_error: 0.0079\n",
      "Epoch 00168: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0047e-04 - mean_absolute_error: 0.0079 - val_loss: 2.8008e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 169/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3743e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00169: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.3743e-05 - mean_absolute_error: 0.0078 - val_loss: 2.2347e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 170/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.1424e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00170: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.1424e-05 - mean_absolute_error: 0.0076 - val_loss: 2.9220e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 171/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.8467e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00171: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.8467e-05 - mean_absolute_error: 0.0080 - val_loss: 4.7327e-05 - val_mean_absolute_error: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2438e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00172: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.2438e-05 - mean_absolute_error: 0.0080 - val_loss: 1.3715e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 173/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2421e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00173: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.2421e-05 - mean_absolute_error: 0.0080 - val_loss: 1.1493e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 174/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1429e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00174: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 8.1429e-05 - mean_absolute_error: 0.0075 - val_loss: 1.1712e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 175/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.4505e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00175: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.4505e-05 - mean_absolute_error: 0.0077 - val_loss: 3.2329e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 176/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.9102e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00176: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.9102e-05 - mean_absolute_error: 0.0077 - val_loss: 2.2139e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 177/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5203e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00177: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.5203e-05 - mean_absolute_error: 0.0081 - val_loss: 1.6185e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 178/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0422e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00178: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 1.0422e-04 - mean_absolute_error: 0.0082 - val_loss: 1.7288e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 179/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0213e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00179: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 9.0213e-05 - mean_absolute_error: 0.0078 - val_loss: 5.8282e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 180/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0099e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00180: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0099e-04 - mean_absolute_error: 0.0081 - val_loss: 3.9180e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 181/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0099e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00181: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0099e-04 - mean_absolute_error: 0.0082 - val_loss: 1.2177e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 182/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 1.0362e-04 - mean_absolute_error: 0.0080\n",
      "Epoch 00182: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 1.0362e-04 - mean_absolute_error: 0.0080 - val_loss: 2.4761e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 183/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0498e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00183: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.0498e-05 - mean_absolute_error: 0.0078 - val_loss: 4.6219e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 184/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.1717e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00184: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 9.1717e-05 - mean_absolute_error: 0.0078 - val_loss: 2.6996e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 185/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.6109e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00185: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 8.6109e-05 - mean_absolute_error: 0.0077 - val_loss: 1.1025e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 186/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7271e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00186: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.7271e-05 - mean_absolute_error: 0.0075 - val_loss: 3.5423e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 187/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0136e-05 - mean_absolute_error: 0.0076- ETA: 0s - loss: 8.0268e-05 - mean_absolute_error: 0.007\n",
      "Epoch 00187: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.0136e-05 - mean_absolute_error: 0.0076 - val_loss: 4.9142e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 188/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5704e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00188: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.5704e-05 - mean_absolute_error: 0.0079 - val_loss: 3.7489e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 189/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.9197e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00189: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.9197e-05 - mean_absolute_error: 0.0080 - val_loss: 1.1560e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 190/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2499e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00190: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.2499e-05 - mean_absolute_error: 0.0079 - val_loss: 1.2648e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 191/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3322e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00191: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.3322e-05 - mean_absolute_error: 0.0076 - val_loss: 6.3737e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 192/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.9625e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00192: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.9625e-05 - mean_absolute_error: 0.0081 - val_loss: 1.7382e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 193/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5730e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00193: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.5730e-05 - mean_absolute_error: 0.0076 - val_loss: 2.0690e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 194/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6876e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00194: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 114ms/step - loss: 9.6876e-05 - mean_absolute_error: 0.0080 - val_loss: 4.3690e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 195/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.8461e-05 - mean_absolute_error: 0.0081\n",
      "Epoch 00195: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 109ms/step - loss: 9.8461e-05 - mean_absolute_error: 0.0081 - val_loss: 3.4324e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 196/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7122e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00196: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.7122e-05 - mean_absolute_error: 0.0075 - val_loss: 1.8758e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 197/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.6825e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00197: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 8.6825e-05 - mean_absolute_error: 0.0077 - val_loss: 1.4348e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 198/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.3683e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00198: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.3683e-05 - mean_absolute_error: 0.0077 - val_loss: 2.9536e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 199/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7332e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00199: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 9.7332e-05 - mean_absolute_error: 0.0080 - val_loss: 1.3632e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 200/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0014e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00200: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 9.0014e-05 - mean_absolute_error: 0.0076 - val_loss: 3.2449e-05 - val_mean_absolute_error: 0.0039\n",
      "Epoch 201/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6648e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00201: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 9.6648e-05 - mean_absolute_error: 0.0080 - val_loss: 1.4249e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 202/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.4667e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00202: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 110ms/step - loss: 9.4667e-05 - mean_absolute_error: 0.0077 - val_loss: 1.2053e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 203/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0970e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00203: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 8.0970e-05 - mean_absolute_error: 0.0074 - val_loss: 1.1770e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 204/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.3504e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00204: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 8.3504e-05 - mean_absolute_error: 0.0075 - val_loss: 1.6196e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 205/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5178e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00205: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 9.5178e-05 - mean_absolute_error: 0.0078 - val_loss: 1.5733e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 206/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.6167e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00206: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 141ms/step - loss: 8.6167e-05 - mean_absolute_error: 0.0076 - val_loss: 1.8527e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 207/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5906e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00207: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 7.5906e-05 - mean_absolute_error: 0.0073 - val_loss: 1.6017e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 208/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0459e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00208: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 143ms/step - loss: 8.0459e-05 - mean_absolute_error: 0.0073 - val_loss: 5.6557e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 209/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8094e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00209: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 147ms/step - loss: 8.8094e-05 - mean_absolute_error: 0.0076 - val_loss: 1.6860e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 210/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.6661e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00210: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 22s 176ms/step - loss: 8.6661e-05 - mean_absolute_error: 0.0075 - val_loss: 5.9259e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 211/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2919e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00211: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 157ms/step - loss: 9.2919e-05 - mean_absolute_error: 0.0078 - val_loss: 3.8192e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 212/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.5605e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00212: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 153ms/step - loss: 9.5605e-05 - mean_absolute_error: 0.0080 - val_loss: 2.2307e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 213/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.3507e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00213: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 8.3507e-05 - mean_absolute_error: 0.0075 - val_loss: 2.1174e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 214/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.4147e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00214: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 147ms/step - loss: 9.4147e-05 - mean_absolute_error: 0.0079 - val_loss: 1.4329e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 215/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2098e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00215: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 8.2098e-05 - mean_absolute_error: 0.0074 - val_loss: 1.6590e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 216/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0857e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00216: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 19s 155ms/step - loss: 8.0857e-05 - mean_absolute_error: 0.0073 - val_loss: 1.0813e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 217/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1895e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00217: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 8.1895e-05 - mean_absolute_error: 0.0073 - val_loss: 4.8509e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 218/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9387e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00218: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 7.9387e-05 - mean_absolute_error: 0.0074 - val_loss: 3.7810e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 219/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1729e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00219: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 8.1729e-05 - mean_absolute_error: 0.0076 - val_loss: 2.3696e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 220/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.9687e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00220: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 141ms/step - loss: 8.9687e-05 - mean_absolute_error: 0.0075 - val_loss: 2.3201e-05 - val_mean_absolute_error: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7548e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00221: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 138ms/step - loss: 7.7548e-05 - mean_absolute_error: 0.0073 - val_loss: 1.8379e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 222/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3527e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00222: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 138ms/step - loss: 9.3527e-05 - mean_absolute_error: 0.0078 - val_loss: 3.0607e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 223/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.9478e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00223: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 135ms/step - loss: 8.9478e-05 - mean_absolute_error: 0.0076 - val_loss: 3.8150e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 224/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4182e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00224: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 8.4182e-05 - mean_absolute_error: 0.0076 - val_loss: 1.2482e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 225/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0842e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00225: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 9.0842e-05 - mean_absolute_error: 0.0076 - val_loss: 2.9182e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 226/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0924e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00226: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 8.0924e-05 - mean_absolute_error: 0.0074 - val_loss: 1.7505e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 227/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8122e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00227: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 124ms/step - loss: 8.8122e-05 - mean_absolute_error: 0.0077 - val_loss: 2.0732e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 228/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9399e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00228: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 129ms/step - loss: 7.9399e-05 - mean_absolute_error: 0.0072 - val_loss: 2.4851e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 229/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4348e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00229: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 8.4348e-05 - mean_absolute_error: 0.0075 - val_loss: 1.9667e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 230/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2148e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00230: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 130ms/step - loss: 8.2148e-05 - mean_absolute_error: 0.0072 - val_loss: 7.0289e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 231/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9855e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00231: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 135ms/step - loss: 7.9855e-05 - mean_absolute_error: 0.0074 - val_loss: 2.2949e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 232/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1295e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00232: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 126ms/step - loss: 8.1295e-05 - mean_absolute_error: 0.0073 - val_loss: 2.8048e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 233/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8758e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00233: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 132ms/step - loss: 7.8758e-05 - mean_absolute_error: 0.0073 - val_loss: 5.0808e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 234/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8569e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00234: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 148ms/step - loss: 7.8569e-05 - mean_absolute_error: 0.0074 - val_loss: 2.1538e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 235/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2650e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00235: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 137ms/step - loss: 8.2650e-05 - mean_absolute_error: 0.0073 - val_loss: 2.5628e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 236/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7430e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00236: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 24s 197ms/step - loss: 8.7430e-05 - mean_absolute_error: 0.0074 - val_loss: 1.3746e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 237/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2441e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00237: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 24s 197ms/step - loss: 9.2441e-05 - mean_absolute_error: 0.0077 - val_loss: 5.7393e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 238/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7570e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00238: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 25s 202ms/step - loss: 8.7570e-05 - mean_absolute_error: 0.0077 - val_loss: 2.4707e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 239/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1595e-05 - mean_absolute_error: 0.0072- ETA: 4s - loss: 7.9192e-05 -\n",
      "Epoch 00239: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 23s 188ms/step - loss: 8.1595e-05 - mean_absolute_error: 0.0072 - val_loss: 2.9016e-05 - val_mean_absolute_error: 0.0039\n",
      "Epoch 240/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9943e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00240: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 7.9943e-05 - mean_absolute_error: 0.0074 - val_loss: 1.8676e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 241/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7165e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00241: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 151ms/step - loss: 8.7165e-05 - mean_absolute_error: 0.0075 - val_loss: 4.6105e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 242/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.3045e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00242: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 22s 175ms/step - loss: 9.3045e-05 - mean_absolute_error: 0.0077 - val_loss: 1.0384e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 243/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7661e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00243: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 7.7661e-05 - mean_absolute_error: 0.0074 - val_loss: 2.1589e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 244/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6542e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00244: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 153ms/step - loss: 7.6542e-05 - mean_absolute_error: 0.0074 - val_loss: 5.0571e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 245/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8081e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00245: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 7.8081e-05 - mean_absolute_error: 0.0072 - val_loss: 1.1641e-05 - val_mean_absolute_error: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7093e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00246: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 28s 223ms/step - loss: 7.7093e-05 - mean_absolute_error: 0.0072 - val_loss: 2.1712e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 247/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4256e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00247: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 25s 202ms/step - loss: 8.4256e-05 - mean_absolute_error: 0.0072 - val_loss: 1.1924e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 248/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.6096e-05 - mean_absolute_error: 0.0079\n",
      "Epoch 00248: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 26s 208ms/step - loss: 9.6096e-05 - mean_absolute_error: 0.0079 - val_loss: 2.3047e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 249/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7936e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00249: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 23s 185ms/step - loss: 7.7936e-05 - mean_absolute_error: 0.0073 - val_loss: 1.4821e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 250/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5510e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00250: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 25s 203ms/step - loss: 8.5510e-05 - mean_absolute_error: 0.0075 - val_loss: 1.3356e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 251/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4702e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00251: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 155ms/step - loss: 8.4702e-05 - mean_absolute_error: 0.0075 - val_loss: 1.0504e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 252/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8614e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00252: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 125ms/step - loss: 7.8614e-05 - mean_absolute_error: 0.0072 - val_loss: 3.3404e-05 - val_mean_absolute_error: 0.0034\n",
      "Epoch 253/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6181e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00253: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 173ms/step - loss: 7.6181e-05 - mean_absolute_error: 0.0073 - val_loss: 1.6884e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 254/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.6703e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00254: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 8.6703e-05 - mean_absolute_error: 0.0075 - val_loss: 1.1398e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 255/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1678e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00255: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 8.1678e-05 - mean_absolute_error: 0.0073 - val_loss: 1.4523e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 256/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2464e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00256: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 8.2464e-05 - mean_absolute_error: 0.0073 - val_loss: 3.0034e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 257/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0403e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00257: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 144ms/step - loss: 8.0403e-05 - mean_absolute_error: 0.0074 - val_loss: 3.4053e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 258/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.2637e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00258: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 9.2637e-05 - mean_absolute_error: 0.0076 - val_loss: 2.4411e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 259/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0778e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00259: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 145ms/step - loss: 8.0778e-05 - mean_absolute_error: 0.0072 - val_loss: 8.6202e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 260/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.7217e-05 - mean_absolute_error: 0.0080\n",
      "Epoch 00260: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 152ms/step - loss: 9.7217e-05 - mean_absolute_error: 0.0080 - val_loss: 1.8334e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 261/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0332e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00261: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 18s 146ms/step - loss: 8.0332e-05 - mean_absolute_error: 0.0073 - val_loss: 9.9986e-06 - val_mean_absolute_error: 0.0024\n",
      "Epoch 262/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2142e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00262: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 142ms/step - loss: 8.2142e-05 - mean_absolute_error: 0.0073 - val_loss: 1.8831e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 263/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6025e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00263: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 149ms/step - loss: 7.6025e-05 - mean_absolute_error: 0.0072 - val_loss: 2.7514e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 264/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0902e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00264: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 150ms/step - loss: 7.0902e-05 - mean_absolute_error: 0.0072 - val_loss: 2.1039e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 265/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9573e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00265: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 7.9573e-05 - mean_absolute_error: 0.0074 - val_loss: 4.0854e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 266/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.4155e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00266: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 157ms/step - loss: 7.4155e-05 - mean_absolute_error: 0.0070 - val_loss: 1.6758e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 267/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9432e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00267: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 139ms/step - loss: 7.9432e-05 - mean_absolute_error: 0.0074 - val_loss: 2.4626e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 268/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0643e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00268: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 151ms/step - loss: 8.0643e-05 - mean_absolute_error: 0.0073 - val_loss: 1.7711e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 269/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.3221e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00269: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 123ms/step - loss: 8.3221e-05 - mean_absolute_error: 0.0072 - val_loss: 3.4319e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 270/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7663e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00270: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 121ms/step - loss: 8.7663e-05 - mean_absolute_error: 0.0076 - val_loss: 3.1915e-05 - val_mean_absolute_error: 0.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.0477e-05 - mean_absolute_error: 0.0078\n",
      "Epoch 00271: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 130ms/step - loss: 9.0477e-05 - mean_absolute_error: 0.0078 - val_loss: 2.4826e-05 - val_mean_absolute_error: 0.0035\n",
      "Epoch 272/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1393e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00272: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 153ms/step - loss: 8.1393e-05 - mean_absolute_error: 0.0072 - val_loss: 2.3854e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 273/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5350e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00273: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 146ms/step - loss: 8.5350e-05 - mean_absolute_error: 0.0076 - val_loss: 3.7487e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 274/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.4414e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00274: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 169ms/step - loss: 7.4414e-05 - mean_absolute_error: 0.0071 - val_loss: 1.5424e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 275/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1609e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00275: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 7.1609e-05 - mean_absolute_error: 0.0071 - val_loss: 1.1297e-05 - val_mean_absolute_error: 0.0028\n",
      "Epoch 276/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3650e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00276: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 118ms/step - loss: 7.3650e-05 - mean_absolute_error: 0.0071 - val_loss: 1.2307e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 277/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1548e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00277: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 8.1548e-05 - mean_absolute_error: 0.0073 - val_loss: 1.5416e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 278/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4149e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00278: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 156ms/step - loss: 8.4149e-05 - mean_absolute_error: 0.0075 - val_loss: 1.4902e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 279/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5203e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00279: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 25s 200ms/step - loss: 7.5203e-05 - mean_absolute_error: 0.0069 - val_loss: 1.8023e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 280/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1215e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00280: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 25s 204ms/step - loss: 8.1215e-05 - mean_absolute_error: 0.0071 - val_loss: 3.9197e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 281/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1213e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00281: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 150ms/step - loss: 8.1213e-05 - mean_absolute_error: 0.0072 - val_loss: 1.4196e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 282/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8328e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00282: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 137ms/step - loss: 7.8328e-05 - mean_absolute_error: 0.0072 - val_loss: 1.8224e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 283/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2353e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00283: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 122ms/step - loss: 8.2353e-05 - mean_absolute_error: 0.0073 - val_loss: 1.4520e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 284/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9492e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00284: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 7.9492e-05 - mean_absolute_error: 0.0073 - val_loss: 3.1317e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 285/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0893e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00285: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 125ms/step - loss: 7.0893e-05 - mean_absolute_error: 0.0070 - val_loss: 1.0551e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 286/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5119e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00286: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 122ms/step - loss: 8.5119e-05 - mean_absolute_error: 0.0075 - val_loss: 1.5311e-05 - val_mean_absolute_error: 0.0035\n",
      "Epoch 287/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0553e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00287: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 147ms/step - loss: 8.0553e-05 - mean_absolute_error: 0.0071 - val_loss: 3.2610e-05 - val_mean_absolute_error: 0.0035\n",
      "Epoch 288/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9504e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00288: val_loss improved from 0.00001 to 0.00001, saving model to results/2020-07-19_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 7.9504e-05 - mean_absolute_error: 0.0072 - val_loss: 9.0705e-06 - val_mean_absolute_error: 0.0024\n",
      "Epoch 289/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6192e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00289: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 121ms/step - loss: 7.6192e-05 - mean_absolute_error: 0.0070 - val_loss: 1.8512e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 290/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.8686e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00290: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 135ms/step - loss: 8.8686e-05 - mean_absolute_error: 0.0077 - val_loss: 2.1461e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 291/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4684e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00291: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 141ms/step - loss: 8.4684e-05 - mean_absolute_error: 0.0074 - val_loss: 1.5048e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 292/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3739e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00292: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 20s 160ms/step - loss: 7.3739e-05 - mean_absolute_error: 0.0071 - val_loss: 1.9614e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 293/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.6816e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00293: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 18s 143ms/step - loss: 8.6816e-05 - mean_absolute_error: 0.0074 - val_loss: 4.1376e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 294/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.7937e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00294: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 8.7937e-05 - mean_absolute_error: 0.0077 - val_loss: 2.5153e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 295/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8833e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00295: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 150ms/step - loss: 7.8833e-05 - mean_absolute_error: 0.0072 - val_loss: 2.2512e-05 - val_mean_absolute_error: 0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5811e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00296: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 8.5811e-05 - mean_absolute_error: 0.0075 - val_loss: 1.1513e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 297/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.8185e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00297: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 141ms/step - loss: 6.8185e-05 - mean_absolute_error: 0.0069 - val_loss: 1.2410e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 298/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5957e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00298: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 15s 119ms/step - loss: 7.5957e-05 - mean_absolute_error: 0.0070 - val_loss: 1.1797e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 299/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7656e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00299: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 151ms/step - loss: 7.7656e-05 - mean_absolute_error: 0.0071 - val_loss: 1.6716e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 300/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8686e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00300: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 172ms/step - loss: 7.8686e-05 - mean_absolute_error: 0.0072 - val_loss: 1.2279e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 301/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7803e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00301: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 167ms/step - loss: 7.7803e-05 - mean_absolute_error: 0.0072 - val_loss: 4.3973e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 302/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6960e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00302: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 166ms/step - loss: 7.6960e-05 - mean_absolute_error: 0.0071 - val_loss: 1.2382e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 303/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7473e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00303: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 168ms/step - loss: 7.7473e-05 - mean_absolute_error: 0.0072 - val_loss: 9.7224e-06 - val_mean_absolute_error: 0.0022\n",
      "Epoch 304/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2882e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00304: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 22s 177ms/step - loss: 7.2882e-05 - mean_absolute_error: 0.0071 - val_loss: 2.9462e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 305/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.4882e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00305: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 21s 168ms/step - loss: 7.4882e-05 - mean_absolute_error: 0.0070 - val_loss: 2.1126e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 306/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.8115e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00306: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 155ms/step - loss: 7.8115e-05 - mean_absolute_error: 0.0072 - val_loss: 3.2923e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 307/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9162e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00307: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 6.9162e-05 - mean_absolute_error: 0.0068 - val_loss: 1.9725e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 308/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3880e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00308: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 141ms/step - loss: 7.3880e-05 - mean_absolute_error: 0.0070 - val_loss: 9.1096e-06 - val_mean_absolute_error: 0.0023\n",
      "Epoch 309/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1089e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00309: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 139ms/step - loss: 7.1089e-05 - mean_absolute_error: 0.0070 - val_loss: 1.6016e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 310/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5945e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00310: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 8.5945e-05 - mean_absolute_error: 0.0074 - val_loss: 6.1386e-05 - val_mean_absolute_error: 0.0039\n",
      "Epoch 311/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.1517e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00311: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 19s 149ms/step - loss: 9.1517e-05 - mean_absolute_error: 0.0076 - val_loss: 1.2645e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 312/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3521e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00312: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 134ms/step - loss: 7.3521e-05 - mean_absolute_error: 0.0070 - val_loss: 1.7510e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 313/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.6884e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00313: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 132ms/step - loss: 6.6884e-05 - mean_absolute_error: 0.0067 - val_loss: 1.8884e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 314/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.4143e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00314: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 16s 133ms/step - loss: 7.4143e-05 - mean_absolute_error: 0.0070 - val_loss: 1.6203e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 315/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3986e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00315: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 137ms/step - loss: 7.3986e-05 - mean_absolute_error: 0.0069 - val_loss: 2.8498e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 316/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.4570e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00316: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 136ms/step - loss: 7.4570e-05 - mean_absolute_error: 0.0071 - val_loss: 2.4068e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 317/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9472e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00317: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 133ms/step - loss: 7.9472e-05 - mean_absolute_error: 0.0071 - val_loss: 1.4086e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 318/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2167e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00318: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 17s 133ms/step - loss: 8.2167e-05 - mean_absolute_error: 0.0076 - val_loss: 1.8730e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 319/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 9.1750e-05 - mean_absolute_error: 0.0076\n",
      "Epoch 00319: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 9.1750e-05 - mean_absolute_error: 0.0076 - val_loss: 1.6421e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 320/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7006e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00320: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.7006e-05 - mean_absolute_error: 0.0072 - val_loss: 3.9717e-05 - val_mean_absolute_error: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2339e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00321: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.2339e-05 - mean_absolute_error: 0.0069 - val_loss: 2.5370e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 322/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9647e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00322: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.9647e-05 - mean_absolute_error: 0.0071 - val_loss: 1.6229e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 323/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5943e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00323: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.5943e-05 - mean_absolute_error: 0.0071 - val_loss: 5.1140e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 324/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.2463e-05 - mean_absolute_error: 0.0074\n",
      "Epoch 00324: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 8.2463e-05 - mean_absolute_error: 0.0074 - val_loss: 1.4531e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 325/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9969e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00325: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.9969e-05 - mean_absolute_error: 0.0072 - val_loss: 3.1827e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 326/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.5375e-05 - mean_absolute_error: 0.0075\n",
      "Epoch 00326: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.5375e-05 - mean_absolute_error: 0.0075 - val_loss: 3.3170e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 327/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.1422e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00327: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 8.1422e-05 - mean_absolute_error: 0.0073 - val_loss: 4.0014e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 328/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1667e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00328: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1667e-05 - mean_absolute_error: 0.0068 - val_loss: 9.2528e-06 - val_mean_absolute_error: 0.0025\n",
      "Epoch 329/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5034e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00329: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 114ms/step - loss: 7.5034e-05 - mean_absolute_error: 0.0071 - val_loss: 1.4301e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 330/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.5427e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00330: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.5427e-05 - mean_absolute_error: 0.0069 - val_loss: 2.0479e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 331/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2813e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00331: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.2813e-05 - mean_absolute_error: 0.0069 - val_loss: 1.1278e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 332/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0191e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00332: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.0191e-05 - mean_absolute_error: 0.0068 - val_loss: 1.2343e-05 - val_mean_absolute_error: 0.0028\n",
      "Epoch 333/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1276e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00333: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1276e-05 - mean_absolute_error: 0.0069 - val_loss: 2.8244e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 334/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3728e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00334: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.3728e-05 - mean_absolute_error: 0.0070 - val_loss: 5.7945e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 335/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9814e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00335: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.9814e-05 - mean_absolute_error: 0.0072 - val_loss: 1.1229e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 336/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0235e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00336: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 7.0235e-05 - mean_absolute_error: 0.0068 - val_loss: 1.0320e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 337/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0797e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00337: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.0797e-05 - mean_absolute_error: 0.0071 - val_loss: 1.4019e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 338/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9659e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00338: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.9659e-05 - mean_absolute_error: 0.0069 - val_loss: 1.4862e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 339/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7706e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00339: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.7706e-05 - mean_absolute_error: 0.0069 - val_loss: 2.1370e-05 - val_mean_absolute_error: 0.0035\n",
      "Epoch 340/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5056e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00340: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.5056e-05 - mean_absolute_error: 0.0071 - val_loss: 1.3144e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 341/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2530e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00341: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.2530e-05 - mean_absolute_error: 0.0069 - val_loss: 2.4209e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 342/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0751e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00342: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.0751e-05 - mean_absolute_error: 0.0070 - val_loss: 1.6130e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 343/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0125e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00343: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.0125e-05 - mean_absolute_error: 0.0072 - val_loss: 1.4869e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 344/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1649e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00344: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1649e-05 - mean_absolute_error: 0.0067 - val_loss: 9.1229e-06 - val_mean_absolute_error: 0.0020\n",
      "Epoch 345/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2125e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00345: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.2125e-05 - mean_absolute_error: 0.0068 - val_loss: 1.1422e-05 - val_mean_absolute_error: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0049e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00346: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.0049e-05 - mean_absolute_error: 0.0070 - val_loss: 1.4244e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 347/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6592e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00347: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.6592e-05 - mean_absolute_error: 0.0071 - val_loss: 1.3708e-05 - val_mean_absolute_error: 0.0021\n",
      "Epoch 348/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9953e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00348: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 105ms/step - loss: 6.9953e-05 - mean_absolute_error: 0.0069 - val_loss: 1.3584e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 349/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7497e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00349: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.7497e-05 - mean_absolute_error: 0.0067 - val_loss: 3.4372e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 350/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6052e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00350: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.6052e-05 - mean_absolute_error: 0.0070 - val_loss: 3.0298e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 351/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.8172e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00351: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.8172e-05 - mean_absolute_error: 0.0067 - val_loss: 1.3822e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 352/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0468e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00352: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.0468e-05 - mean_absolute_error: 0.0070 - val_loss: 1.1847e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 353/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5157e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00353: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.5157e-05 - mean_absolute_error: 0.0068 - val_loss: 2.1925e-05 - val_mean_absolute_error: 0.0039\n",
      "Epoch 354/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0923e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00354: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 8.0923e-05 - mean_absolute_error: 0.0072 - val_loss: 1.5821e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 355/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.6844e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00355: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 6.6844e-05 - mean_absolute_error: 0.0070 - val_loss: 1.1441e-05 - val_mean_absolute_error: 0.0027\n",
      "Epoch 356/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9426e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00356: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.9426e-05 - mean_absolute_error: 0.0073 - val_loss: 2.0699e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 357/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9003e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00357: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.9003e-05 - mean_absolute_error: 0.0073 - val_loss: 1.6797e-05 - val_mean_absolute_error: 0.0035\n",
      "Epoch 358/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7551e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00358: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 6.7551e-05 - mean_absolute_error: 0.0068 - val_loss: 3.4251e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 359/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2948e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00359: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.2948e-05 - mean_absolute_error: 0.0070 - val_loss: 9.5138e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 360/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.8392e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00360: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.8392e-05 - mean_absolute_error: 0.0068 - val_loss: 1.3178e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 361/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7546e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00361: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.7546e-05 - mean_absolute_error: 0.0067 - val_loss: 1.3125e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 362/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1752e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00362: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1752e-05 - mean_absolute_error: 0.0068 - val_loss: 1.2780e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 363/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0963e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00363: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.0963e-05 - mean_absolute_error: 0.0069 - val_loss: 1.9124e-05 - val_mean_absolute_error: 0.0033\n",
      "Epoch 364/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1707e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00364: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1707e-05 - mean_absolute_error: 0.0068 - val_loss: 2.1297e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 365/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.4896e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00365: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.4896e-05 - mean_absolute_error: 0.0069 - val_loss: 1.5516e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 366/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9553e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00366: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.9553e-05 - mean_absolute_error: 0.0069 - val_loss: 2.3849e-05 - val_mean_absolute_error: 0.0035\n",
      "Epoch 367/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1415e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00367: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.1415e-05 - mean_absolute_error: 0.0067 - val_loss: 4.1204e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 368/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9986e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00368: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 6.9986e-05 - mean_absolute_error: 0.0069 - val_loss: 1.5198e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 369/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1848e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00369: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1848e-05 - mean_absolute_error: 0.0068 - val_loss: 1.6087e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 370/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7066e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00370: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.7066e-05 - mean_absolute_error: 0.0067 - val_loss: 2.1192e-05 - val_mean_absolute_error: 0.0037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3937e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00371: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.3937e-05 - mean_absolute_error: 0.0070 - val_loss: 1.3797e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 372/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7805e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00372: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.7805e-05 - mean_absolute_error: 0.0068 - val_loss: 1.3900e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 373/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.6548e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00373: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 7.6548e-05 - mean_absolute_error: 0.0072 - val_loss: 3.6915e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 374/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1029e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00374: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 112ms/step - loss: 7.1029e-05 - mean_absolute_error: 0.0068 - val_loss: 2.6678e-05 - val_mean_absolute_error: 0.0038\n",
      "Epoch 375/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1567e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00375: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.1567e-05 - mean_absolute_error: 0.0070 - val_loss: 3.2184e-05 - val_mean_absolute_error: 0.0036\n",
      "Epoch 376/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2511e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00376: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.2511e-05 - mean_absolute_error: 0.0069 - val_loss: 6.3422e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 377/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5944e-05 - mean_absolute_error: 0.0072\n",
      "Epoch 00377: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.5944e-05 - mean_absolute_error: 0.0072 - val_loss: 1.1073e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 378/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3941e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00378: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.3941e-05 - mean_absolute_error: 0.0069 - val_loss: 1.2553e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 379/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.4251e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00379: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 6.4251e-05 - mean_absolute_error: 0.0067 - val_loss: 2.0904e-05 - val_mean_absolute_error: 0.0030\n",
      "Epoch 380/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3121e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00380: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.3121e-05 - mean_absolute_error: 0.0069 - val_loss: 2.6224e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 381/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7470e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00381: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 14s 111ms/step - loss: 7.7470e-05 - mean_absolute_error: 0.0071 - val_loss: 1.7152e-05 - val_mean_absolute_error: 0.0037\n",
      "Epoch 382/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9487e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00382: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 7.9487e-05 - mean_absolute_error: 0.0073 - val_loss: 2.4116e-05 - val_mean_absolute_error: 0.0032\n",
      "Epoch 383/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5517e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00383: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 108ms/step - loss: 7.5517e-05 - mean_absolute_error: 0.0070 - val_loss: 1.0821e-05 - val_mean_absolute_error: 0.0024\n",
      "Epoch 384/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.7939e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 00384: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.7939e-05 - mean_absolute_error: 0.0071 - val_loss: 1.2051e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 385/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.0625e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00385: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.0625e-05 - mean_absolute_error: 0.0067 - val_loss: 1.3254e-05 - val_mean_absolute_error: 0.0025\n",
      "Epoch 386/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3101e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00386: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.3101e-05 - mean_absolute_error: 0.0068 - val_loss: 2.5304e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 387/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1738e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00387: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.1738e-05 - mean_absolute_error: 0.0070 - val_loss: 3.5504e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 388/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.4086e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00388: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.4086e-05 - mean_absolute_error: 0.0073 - val_loss: 1.2149e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 389/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 8.0181e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00389: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 8.0181e-05 - mean_absolute_error: 0.0070 - val_loss: 2.1103e-05 - val_mean_absolute_error: 0.0029\n",
      "Epoch 390/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.5456e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 00390: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.5456e-05 - mean_absolute_error: 0.0067 - val_loss: 2.2817e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 391/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.9103e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00391: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 105ms/step - loss: 6.9103e-05 - mean_absolute_error: 0.0069 - val_loss: 1.1000e-05 - val_mean_absolute_error: 0.0023\n",
      "Epoch 392/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.3666e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00392: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.3666e-05 - mean_absolute_error: 0.0070 - val_loss: 1.4190e-05 - val_mean_absolute_error: 0.0026\n",
      "Epoch 393/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.5388e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00393: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.5388e-05 - mean_absolute_error: 0.0070 - val_loss: 2.6073e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 394/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.8651e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00394: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 6.8651e-05 - mean_absolute_error: 0.0069 - val_loss: 1.5258e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 395/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.9010e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00395: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.9010e-05 - mean_absolute_error: 0.0069 - val_loss: 2.9250e-05 - val_mean_absolute_error: 0.0042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1783e-05 - mean_absolute_error: 0.0070\n",
      "Epoch 00396: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1783e-05 - mean_absolute_error: 0.0070 - val_loss: 3.6645e-05 - val_mean_absolute_error: 0.0040\n",
      "Epoch 397/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.6473e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00397: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 6.6473e-05 - mean_absolute_error: 0.0069 - val_loss: 1.1371e-05 - val_mean_absolute_error: 0.0022\n",
      "Epoch 398/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.1367e-05 - mean_absolute_error: 0.0070- ETA: 2s - loss: 6.9283e-05 - mean\n",
      "Epoch 00398: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 7.1367e-05 - mean_absolute_error: 0.0070 - val_loss: 1.6135e-05 - val_mean_absolute_error: 0.0031\n",
      "Epoch 399/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 7.2139e-05 - mean_absolute_error: 0.0069\n",
      "Epoch 00399: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 7.2139e-05 - mean_absolute_error: 0.0069 - val_loss: 1.7773e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 400/400\n",
      "124/124 [==============================] - ETA: 0s - loss: 6.7956e-05 - mean_absolute_error: 0.0068\n",
      "Epoch 00400: val_loss did not improve from 0.00001\n",
      "124/124 [==============================] - 13s 107ms/step - loss: 6.7956e-05 - mean_absolute_error: 0.0068 - val_loss: 9.8904e-06 - val_mean_absolute_error: 0.0026\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard,es],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.424268679294879\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, classification=False):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
    "    # retrieve the column scalers\n",
    "    column_scaler = data[\"column_scaler\"]\n",
    "    # reshape the last sequence\n",
    "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 398.45$\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    # last 200 days, feel free to edit that\n",
    "    plt.plot(y_test[-200:], c='b')\n",
    "    plt.plot(y_pred[-200:], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTMs to predict the closing stock price of Apple Inc. (Using past 60 days data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import pandas_datareader as web\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>111.440002</td>\n",
       "      <td>107.349998</td>\n",
       "      <td>111.389999</td>\n",
       "      <td>109.330002</td>\n",
       "      <td>53204600.0</td>\n",
       "      <td>99.945885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>108.650002</td>\n",
       "      <td>105.410004</td>\n",
       "      <td>108.290001</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>64285500.0</td>\n",
       "      <td>97.130241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>107.430000</td>\n",
       "      <td>104.629997</td>\n",
       "      <td>106.540001</td>\n",
       "      <td>106.260002</td>\n",
       "      <td>65797100.0</td>\n",
       "      <td>97.139420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>108.199997</td>\n",
       "      <td>106.699997</td>\n",
       "      <td>107.199997</td>\n",
       "      <td>107.750000</td>\n",
       "      <td>40105900.0</td>\n",
       "      <td>98.501518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>112.150002</td>\n",
       "      <td>108.699997</td>\n",
       "      <td>109.230003</td>\n",
       "      <td>111.889999</td>\n",
       "      <td>59364500.0</td>\n",
       "      <td>102.286186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close      Volume  \\\n",
       "Date                                                                     \n",
       "2015-01-02  111.440002  107.349998  111.389999  109.330002  53204600.0   \n",
       "2015-01-05  108.650002  105.410004  108.290001  106.250000  64285500.0   \n",
       "2015-01-06  107.430000  104.629997  106.540001  106.260002  65797100.0   \n",
       "2015-01-07  108.199997  106.699997  107.199997  107.750000  40105900.0   \n",
       "2015-01-08  112.150002  108.699997  109.230003  111.889999  59364500.0   \n",
       "\n",
       "             Adj Close  \n",
       "Date                    \n",
       "2015-01-02   99.945885  \n",
       "2015-01-05   97.130241  \n",
       "2015-01-06   97.139420  \n",
       "2015-01-07   98.501518  \n",
       "2015-01-08  102.286186  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data from 1 Jan 2015 to 01 Aug 2020\n",
    "df = web.DataReader('AAPL', data_source='yahoo', start = '2015-01-01',end='2020-08-01')\n",
    "# exploring data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing closing price history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.title('Apple closing stock price history')\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Price(USD ($))', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select closing price only\n",
    "data = df.filter(['Close'])\n",
    "# convert to numpy \n",
    "dataset = data.values\n",
    "# compute number of rows to train model on.\n",
    "training_data_len = math.ceil(len(dataset)*0.8)\n",
    "\n",
    "training_data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling prices between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.05673739, 0.04753512, 0.047565  , 0.05201674, 0.06438602,\n",
      "       0.06474456, 0.05649837, 0.05939649, 0.05814164, 0.04923813,\n",
      "       0.04675829, 0.05491486, 0.0573947 , 0.06590978, 0.06764268,\n",
      "       0.0680012 , 0.05616971, 0.07460412, 0.08533016, 0.08013148,\n",
      "       0.08452345, 0.08458322, 0.08730206, 0.08843742, 0.08541979,\n",
      "       0.08778011, 0.09465192, 0.10319689, 0.10791754, 0.10976996,\n",
      "       0.11201077, 0.11466986, 0.11386316, 0.11700031, 0.12745743,\n",
      "       0.12497759, 0.11487898, 0.11974903, 0.11389307, 0.11577532,\n",
      "       0.11658202, 0.11413205, 0.10776817, 0.10833583, 0.10994921,\n",
      "       0.10209144, 0.09530923, 0.10191216, 0.09934269, 0.10340603,\n",
      "       0.10965044, 0.11392293, 0.11102481, 0.10624441, 0.11015835,\n",
      "       0.10860473, 0.09871527, 0.10128473, 0.09832687, 0.10764866])]\n",
      "[0.10185241327747102]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = scaled_data[0:training_data_len, :]\n",
    "# split data into x_train and y_train\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# 0-60 days as features and day 61 as targets\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i,0])\n",
    "    y_train.append(train_data[i,0])\n",
    "    # print data for first pass through\n",
    "    if i<=60:\n",
    "        print(x_train)\n",
    "        print(y_train)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data sets to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1064, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1064, 60, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape x_train since lstm expects 3D array\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "model.save(os.path.join(\"results\", 'myModel') + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-b719c9376ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'keys'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
